{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Import libraries**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Default parameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "MAX_SIZE = 783\n",
    "NUM_WORDS = 1000\n",
    "EMBEDDING_DIM = 16\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "OOV = 0\n",
    "SARCASM_TRAINING_SIZE = 20000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating DataFrames**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "goodreads_train = pd.read_csv(\"kaggle/input/goodreads_train.csv\")\n",
    "goodreads_test = pd.read_csv(\"kaggle/input/goodreads_test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  \\\n620942  ecbfc359e8b4a7d05f549e30f489c501  29405105   \n196121  848fb2c7399065f74f24f2ec15a78759  10369932   \n716590  9f7d458360179a07a6c092778676d651  22826126   \n383182  d8d4cad6c612f47c781143d35255d497  16181775   \n737901  e5e161374b0acd1c063c99508546244b  16101128   \n\n                               review_id  rating  \\\n620942  e7c6afdf00497a24aa129a55e1545cfd       5   \n196121  cca3aa581774f49f6f8a6f63f7b85fc7       3   \n716590  6957d81fd3cb0dd1ff085199488e1e40       4   \n383182  cfcd7cb1b687ecd6a2508ef5f7b30560       5   \n737901  04787f4d58130b0f1473e9c7adfaf5a8       3   \n\n                                              review_text  \\\n620942  5 stars \\n I am always excited when the latest...   \n196121  Perfect for a middle school audience; my book ...   \n716590  Really a 4.5 star review. (Basically, 5 stars ...   \n383182  Good Story #82. Julie and Scott discover that ...   \n737901  I believe I'm a bit too old for young adult bo...   \n\n                            date_added                    date_updated  \\\n620942  Sun Jul 05 23:07:06 -0700 2015  Fri Oct 27 04:12:19 -0700 2017   \n196121  Sun Jun 25 18:53:45 -0700 2017  Tue Jul 11 06:58:43 -0700 2017   \n716590  Thu Sep 07 20:58:02 -0700 2017  Thu Sep 07 21:13:04 -0700 2017   \n383182  Wed Jan 29 13:55:39 -0800 2014  Sat Apr 29 07:54:26 -0700 2017   \n737901  Wed Jul 20 10:36:16 -0700 2016  Wed Jul 20 10:59:08 -0700 2016   \n\n                               read_at                      started_at  \\\n620942  Tue May 24 21:35:03 -0700 2016                             NaN   \n196121  Tue Jul 11 06:58:43 -0700 2017  Sun Jul 09 09:48:40 -0700 2017   \n716590  Wed Sep 06 00:00:00 -0700 2017  Mon Sep 04 00:00:00 -0700 2017   \n383182  Sat Apr 29 07:54:26 -0700 2017  Tue Apr 25 00:00:00 -0700 2017   \n737901  Sat Jul 16 00:00:00 -0700 2016                             NaN   \n\n        n_votes  n_comments  \n620942        0           0  \n196121        0           0  \n716590        1           2  \n383182        5           2  \n737901        0           0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>rating</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>620942</th>\n      <td>ecbfc359e8b4a7d05f549e30f489c501</td>\n      <td>29405105</td>\n      <td>e7c6afdf00497a24aa129a55e1545cfd</td>\n      <td>5</td>\n      <td>5 stars \\n I am always excited when the latest...</td>\n      <td>Sun Jul 05 23:07:06 -0700 2015</td>\n      <td>Fri Oct 27 04:12:19 -0700 2017</td>\n      <td>Tue May 24 21:35:03 -0700 2016</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>196121</th>\n      <td>848fb2c7399065f74f24f2ec15a78759</td>\n      <td>10369932</td>\n      <td>cca3aa581774f49f6f8a6f63f7b85fc7</td>\n      <td>3</td>\n      <td>Perfect for a middle school audience; my book ...</td>\n      <td>Sun Jun 25 18:53:45 -0700 2017</td>\n      <td>Tue Jul 11 06:58:43 -0700 2017</td>\n      <td>Tue Jul 11 06:58:43 -0700 2017</td>\n      <td>Sun Jul 09 09:48:40 -0700 2017</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>716590</th>\n      <td>9f7d458360179a07a6c092778676d651</td>\n      <td>22826126</td>\n      <td>6957d81fd3cb0dd1ff085199488e1e40</td>\n      <td>4</td>\n      <td>Really a 4.5 star review. (Basically, 5 stars ...</td>\n      <td>Thu Sep 07 20:58:02 -0700 2017</td>\n      <td>Thu Sep 07 21:13:04 -0700 2017</td>\n      <td>Wed Sep 06 00:00:00 -0700 2017</td>\n      <td>Mon Sep 04 00:00:00 -0700 2017</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>383182</th>\n      <td>d8d4cad6c612f47c781143d35255d497</td>\n      <td>16181775</td>\n      <td>cfcd7cb1b687ecd6a2508ef5f7b30560</td>\n      <td>5</td>\n      <td>Good Story #82. Julie and Scott discover that ...</td>\n      <td>Wed Jan 29 13:55:39 -0800 2014</td>\n      <td>Sat Apr 29 07:54:26 -0700 2017</td>\n      <td>Sat Apr 29 07:54:26 -0700 2017</td>\n      <td>Tue Apr 25 00:00:00 -0700 2017</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>737901</th>\n      <td>e5e161374b0acd1c063c99508546244b</td>\n      <td>16101128</td>\n      <td>04787f4d58130b0f1473e9c7adfaf5a8</td>\n      <td>3</td>\n      <td>I believe I'm a bit too old for young adult bo...</td>\n      <td>Wed Jul 20 10:36:16 -0700 2016</td>\n      <td>Wed Jul 20 10:59:08 -0700 2016</td>\n      <td>Sat Jul 16 00:00:00 -0700 2016</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodreads_train.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  \\\n201994  50e2ef79c502ae6e28c056eb1c9f9d89  22545870   \n398832  501209dbf1919c8bb0a32351385fc883   9938498   \n152805  1b790a9d988cceacf12b08e6ccc7c2f8   8667848   \n445181  ab89d10cc2d51c1eeb4d5d5077ed7fb9  16096824   \n399345  932971ffd04486a4a3a2c040c7ca32c3  29772863   \n\n                               review_id  \\\n201994  3d7df4ff2abb0235af622a167f819a1f   \n398832  149d776a1dee7b4ccf9f604e5ffe544a   \n152805  141d55bb62a7a08f14fd63eb6b431055   \n445181  3e1b68aafcf725a4bd9ad48ca328efb1   \n399345  b3a50190db53c3cb70dc47e1c590cd23   \n\n                                              review_text  \\\n201994  If I hadn't known any better, I would have ass...   \n398832  I read this as a buddy read with my Goodreads'...   \n152805  All in all. Started really interesting, and th...   \n445181  **RANTISH** \\n This book left me with a bunch ...   \n399345  That was adorably epic!!! I loved the art, sto...   \n\n                            date_added                    date_updated  \\\n201994  Sun Apr 05 16:06:15 -0700 2015  Mon Jun 22 10:05:14 -0700 2015   \n398832  Thu Mar 31 16:28:51 -0700 2011  Mon Aug 22 03:34:38 -0700 2011   \n152805  Mon Jun 13 14:36:58 -0700 2011  Mon Jun 20 07:00:57 -0700 2011   \n445181  Tue Aug 18 02:00:44 -0700 2015  Mon Aug 24 22:01:54 -0700 2015   \n399345  Fri Jan 13 10:17:07 -0800 2017  Sun Feb 05 07:47:18 -0800 2017   \n\n                               read_at                      started_at  \\\n201994  Mon Apr 06 15:33:25 -0700 2015  Sun Apr 05 00:00:00 -0700 2015   \n398832  Mon Aug 22 00:00:00 -0700 2011  Fri Aug 12 00:00:00 -0700 2011   \n152805  Tue Feb 01 00:00:00 -0800 2011                             NaN   \n445181  Sun Aug 23 00:00:00 -0700 2015  Tue Aug 18 00:00:00 -0700 2015   \n399345  Sun Feb 05 07:47:18 -0800 2017  Sat Feb 04 00:00:00 -0800 2017   \n\n        n_votes  n_comments  \n201994        2           0  \n398832        6          39  \n152805        0           0  \n445181        0           0  \n399345        0           0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>201994</th>\n      <td>50e2ef79c502ae6e28c056eb1c9f9d89</td>\n      <td>22545870</td>\n      <td>3d7df4ff2abb0235af622a167f819a1f</td>\n      <td>If I hadn't known any better, I would have ass...</td>\n      <td>Sun Apr 05 16:06:15 -0700 2015</td>\n      <td>Mon Jun 22 10:05:14 -0700 2015</td>\n      <td>Mon Apr 06 15:33:25 -0700 2015</td>\n      <td>Sun Apr 05 00:00:00 -0700 2015</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>398832</th>\n      <td>501209dbf1919c8bb0a32351385fc883</td>\n      <td>9938498</td>\n      <td>149d776a1dee7b4ccf9f604e5ffe544a</td>\n      <td>I read this as a buddy read with my Goodreads'...</td>\n      <td>Thu Mar 31 16:28:51 -0700 2011</td>\n      <td>Mon Aug 22 03:34:38 -0700 2011</td>\n      <td>Mon Aug 22 00:00:00 -0700 2011</td>\n      <td>Fri Aug 12 00:00:00 -0700 2011</td>\n      <td>6</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>152805</th>\n      <td>1b790a9d988cceacf12b08e6ccc7c2f8</td>\n      <td>8667848</td>\n      <td>141d55bb62a7a08f14fd63eb6b431055</td>\n      <td>All in all. Started really interesting, and th...</td>\n      <td>Mon Jun 13 14:36:58 -0700 2011</td>\n      <td>Mon Jun 20 07:00:57 -0700 2011</td>\n      <td>Tue Feb 01 00:00:00 -0800 2011</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>445181</th>\n      <td>ab89d10cc2d51c1eeb4d5d5077ed7fb9</td>\n      <td>16096824</td>\n      <td>3e1b68aafcf725a4bd9ad48ca328efb1</td>\n      <td>**RANTISH** \\n This book left me with a bunch ...</td>\n      <td>Tue Aug 18 02:00:44 -0700 2015</td>\n      <td>Mon Aug 24 22:01:54 -0700 2015</td>\n      <td>Sun Aug 23 00:00:00 -0700 2015</td>\n      <td>Tue Aug 18 00:00:00 -0700 2015</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>399345</th>\n      <td>932971ffd04486a4a3a2c040c7ca32c3</td>\n      <td>29772863</td>\n      <td>b3a50190db53c3cb70dc47e1c590cd23</td>\n      <td>That was adorably epic!!! I loved the art, sto...</td>\n      <td>Fri Jan 13 10:17:07 -0800 2017</td>\n      <td>Sun Feb 05 07:47:18 -0800 2017</td>\n      <td>Sun Feb 05 07:47:18 -0800 2017</td>\n      <td>Sat Feb 04 00:00:00 -0800 2017</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodreads_test.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cleaning Data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                          review_id  rating  \\\n0  dfdbb7b0eb5a7e4c26d59a937e2e5feb       5   \n1  a5d2c3628987712d0e05c4f90798eb67       3   \n2  2ede853b14dc4583f96cf5d120af636f       3   \n3  ced5675e55cd9d38a524743f5c40996e       0   \n4  332732725863131279a8e345b63ac33e       4   \n\n                                         review_text  n_votes  n_comments  \n0  This is a special book. It started slow for ab...       28           1  \n1  Recommended by Don Katz. Avail for free in Dec...        1           0  \n2  A fun, fast paced science fiction thriller. I ...       22           0  \n3  Recommended reading to understand what is goin...        5           1  \n4  I really enjoyed this book, and there is a lot...        9           1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>rating</th>\n      <th>review_text</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n      <td>5</td>\n      <td>This is a special book. It started slow for ab...</td>\n      <td>28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a5d2c3628987712d0e05c4f90798eb67</td>\n      <td>3</td>\n      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2ede853b14dc4583f96cf5d120af636f</td>\n      <td>3</td>\n      <td>A fun, fast paced science fiction thriller. I ...</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ced5675e55cd9d38a524743f5c40996e</td>\n      <td>0</td>\n      <td>Recommended reading to understand what is goin...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>332732725863131279a8e345b63ac33e</td>\n      <td>4</td>\n      <td>I really enjoyed this book, and there is a lot...</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = goodreads_train.drop(columns=['user_id', 'book_id', 'date_added', 'date_updated', 'read_at', 'started_at'],\n",
    "                                axis=0)\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "review_id      object\nrating          int64\nreview_text    object\nn_votes         int64\nn_comments      int64\ndtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y_train = train_df['rating']\n",
    "x_train = train_df.drop('rating', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                          review_id  \\\n0  dfdbb7b0eb5a7e4c26d59a937e2e5feb   \n1  a5d2c3628987712d0e05c4f90798eb67   \n2  2ede853b14dc4583f96cf5d120af636f   \n3  ced5675e55cd9d38a524743f5c40996e   \n4  332732725863131279a8e345b63ac33e   \n\n                                         review_text  n_votes  n_comments  \n0  This is a special book. It started slow for ab...       28           1  \n1  Recommended by Don Katz. Avail for free in Dec...        1           0  \n2  A fun, fast paced science fiction thriller. I ...       22           0  \n3  Recommended reading to understand what is goin...        5           1  \n4  I really enjoyed this book, and there is a lot...        9           1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dfdbb7b0eb5a7e4c26d59a937e2e5feb</td>\n      <td>This is a special book. It started slow for ab...</td>\n      <td>28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a5d2c3628987712d0e05c4f90798eb67</td>\n      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2ede853b14dc4583f96cf5d120af636f</td>\n      <td>A fun, fast paced science fiction thriller. I ...</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ced5675e55cd9d38a524743f5c40996e</td>\n      <td>Recommended reading to understand what is goin...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>332732725863131279a8e345b63ac33e</td>\n      <td>I really enjoyed this book, and there is a lot...</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0    5\n1    3\n2    3\n3    0\n4    4\nName: rating, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "x_train['review_id'] = le.fit_transform(x_train['review_id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   review_id                                        review_text  n_votes  \\\n0     786842  This is a special book. It started slow for ab...       28   \n1     583423  Recommended by Don Katz. Avail for free in Dec...        1   \n2     165147  A fun, fast paced science fiction thriller. I ...       22   \n3     727692  Recommended reading to understand what is goin...        5   \n4     179941  I really enjoyed this book, and there is a lot...        9   \n\n   n_comments  \n0           1  \n1           0  \n2           0  \n3           1  \n4           1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>786842</td>\n      <td>This is a special book. It started slow for ab...</td>\n      <td>28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>583423</td>\n      <td>Recommended by Don Katz. Avail for free in Dec...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165147</td>\n      <td>A fun, fast paced science fiction thriller. I ...</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>727692</td>\n      <td>Recommended reading to understand what is goin...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>179941</td>\n      <td>I really enjoyed this book, and there is a lot...</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NLP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_sequences(tokenizer, review):\n",
    "    sequences = tokenizer.texts_to_sequences(review)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=MAX_SIZE, padding='post')\n",
    "    return padded_sequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def tokenizer_func(data_rating, data_review):\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV)\n",
    "    tokenizer.fit_on_texts(data_review)\n",
    "\n",
    "    \"\"\"word_index = tokenizer.word_index\"\"\"\n",
    "\n",
    "    train_labels = data_rating.iloc[math.floor(int(len(data_rating) / 8)):]\n",
    "    train_examples = data_review.iloc[math.floor(int(len(data_review) / 8)):]\n",
    "    test_examples = data_review.iloc[:math.floor(int(len(data_review) / 8))]\n",
    "    test_labels = data_rating.iloc[:math.floor(int(len(data_rating) / 8))]\n",
    "\n",
    "    padded_train = get_sequences(tokenizer, train_examples)\n",
    "    padded_test = get_sequences(tokenizer, test_examples)\n",
    "\n",
    "    return np.array(padded_train), np.array(padded_test), np.array(train_labels), np.array(test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "padded_train, padded_test, train_labels, test_labels = tokenizer_func(y_train, x_train['review_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  4,  12, 294,  16,   6,   1, 566,  31,   3,   9,  10, 456,  42,\n         4, 151,  88, 597,   1,  59,  10,   6,   1,   7,   6,   1,   1,\n        21,  10, 635,  16,   2,   1,   1, 459,  45,  34, 185,   1,  27,\n         6, 428,   5,   1,   2, 223,   7,   2, 888,   1,   1, 388,  42,\n        80,   4, 134,  47,   1, 362,   3, 566,  32,  42,   4,  12, 294,\n        16,   4,  91,  37, 291,  13,  14,  15,   9, 167, 209, 124, 207,\n        25, 132,  48, 156, 290,   3,  84,   1, 255, 111, 383, 242, 124,\n         3,  42, 180,   7, 460,  10,   1,   1,  24,   2, 434,  70, 914,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Sarcasm detection**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def define_sarcasm():\n",
    "    data = pd.read_json('kaggle/input/sarcasm.json', lines=True)\n",
    "    # iterating through the json data and loading the requisite values into our python lists\n",
    "    sentences = data['headline']\n",
    "    labels = data['is_sarcastic']\n",
    "\n",
    "    training_sentences = sentences[0:SARCASM_TRAINING_SIZE]\n",
    "    testing_sentences = sentences[SARCASM_TRAINING_SIZE:]\n",
    "\n",
    "    training_labels = labels[0:SARCASM_TRAINING_SIZE]\n",
    "    testing_labels = labels[SARCASM_TRAINING_SIZE:]\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV)\n",
    "    # fitting tokenizer only to training set\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # creating training sequences and padding them\n",
    "    traning_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    training_padded = pad_sequences(traning_sequences, maxlen=MAX_SIZE,\n",
    "                                    padding='post',\n",
    "                                    truncating='post',\n",
    "                                    )\n",
    "\n",
    "    # creating  testing sequences and padding them using same tokenizer\n",
    "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=MAX_SIZE,\n",
    "                                   padding='post',\n",
    "                                   truncating='post',\n",
    "                                   )\n",
    "\n",
    "    # converting all variables to numpy arrays, to be able to work with tf version 2\n",
    "    training_padded = np.array(training_padded)\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_padded = np.array(testing_padded)\n",
    "    testing_labels = np.array(testing_labels)\n",
    "\n",
    "    # creating a model for sentiment analysis\n",
    "    model = tf.keras.Sequential([\n",
    "        # addinging an Embedding layer for Neural Network to learn the vectors\n",
    "        tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=MAX_SIZE),\n",
    "        # Global Average pooling is similar to adding up vectors in this case\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \"\"\"model.fit(training_padded, training_labels, epochs=EPOCHS,\n",
    "                        validation_data=(testing_padded, testing_labels))\"\"\"\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "sarcasm_model = define_sarcasm()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24610/24610 [==============================] - 63s 2ms/step\n",
      "3516/3516 [==============================] - 8s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "sarcasm_prediction_train = sarcasm_model.predict(padded_train)\n",
    "sarcasm_prediction_test = sarcasm_model.predict(padded_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reshaping data**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(sarcasm_prediction_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787500, 783)\n"
     ]
    }
   ],
   "source": [
    "print(padded_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "padded_train = np.concatenate((padded_train, np.array(sarcasm_prediction_train.flatten())[:, None]), axis=1)\n",
    "padded_test = np.concatenate((padded_test, np.array(sarcasm_prediction_test.flatten())[:, None]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787500, 784)\n"
     ]
    }
   ],
   "source": [
    "print(padded_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "padded_train = np.reshape(padded_train, (1 - math.floor(len(y_train) / 8), 28, 28))\n",
    "padded_test = np.reshape(padded_test, (math.floor(int(len(x_train['review_text']) / 8)), 28, 28))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.000000e+00 1.200000e+01 2.940000e+02 1.600000e+01 6.000000e+00\n",
      "  1.000000e+00 5.660000e+02 3.100000e+01 3.000000e+00 9.000000e+00\n",
      "  1.000000e+01 4.560000e+02 4.200000e+01 4.000000e+00 1.510000e+02\n",
      "  8.800000e+01 5.970000e+02 1.000000e+00 5.900000e+01 1.000000e+01\n",
      "  6.000000e+00 1.000000e+00 7.000000e+00 6.000000e+00 1.000000e+00\n",
      "  1.000000e+00 2.100000e+01 1.000000e+01]\n",
      " [6.350000e+02 1.600000e+01 2.000000e+00 1.000000e+00 1.000000e+00\n",
      "  4.590000e+02 4.500000e+01 3.400000e+01 1.850000e+02 1.000000e+00\n",
      "  2.700000e+01 6.000000e+00 4.280000e+02 5.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.230000e+02 7.000000e+00 2.000000e+00 8.880000e+02\n",
      "  1.000000e+00 1.000000e+00 3.880000e+02 4.200000e+01 8.000000e+01\n",
      "  4.000000e+00 1.340000e+02 4.700000e+01]\n",
      " [1.000000e+00 3.620000e+02 3.000000e+00 5.660000e+02 3.200000e+01\n",
      "  4.200000e+01 4.000000e+00 1.200000e+01 2.940000e+02 1.600000e+01\n",
      "  4.000000e+00 9.100000e+01 3.700000e+01 2.910000e+02 1.300000e+01\n",
      "  1.400000e+01 1.500000e+01 9.000000e+00 1.670000e+02 2.090000e+02\n",
      "  1.240000e+02 2.070000e+02 2.500000e+01 1.320000e+02 4.800000e+01\n",
      "  1.560000e+02 2.900000e+02 3.000000e+00]\n",
      " [8.400000e+01 1.000000e+00 2.550000e+02 1.110000e+02 3.830000e+02\n",
      "  2.420000e+02 1.240000e+02 3.000000e+00 4.200000e+01 1.800000e+02\n",
      "  7.000000e+00 4.600000e+02 1.000000e+01 1.000000e+00 1.000000e+00\n",
      "  2.400000e+01 2.000000e+00 4.340000e+02 7.000000e+01 9.140000e+02\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 5.052737e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_train[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CNN Training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(8, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(8, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.tanh, padding='same'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(32, activation=tf.keras.activations.relu))  # tf.keras.activations.tanh\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.keras.activations.relu))  # tf.keras.activations.tanh\n",
    "model.add(tf.keras.layers.Dense(6,\n",
    "                                activation=tf.keras.activations.softmax))  # model.add(tf.keras.layers.Dense(1, activation=tf.keras.activations.softmax))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(0.1, momentum=0.9),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[1;32mIn [57]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpadded_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m          \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m          \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m          \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\4IABD1_DL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\4IABD1_DL\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mInternalError\u001B[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "model.fit(padded_train,\n",
    "          train_labels,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS\n",
    "          )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Testing model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_start(padded_train, padded_test, train_labels, test_labels, model):\n",
    "    padded_train = padded_train / 255.0\n",
    "    padded_test = padded_test / 255.0\n",
    "\n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, 6)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels, 6)\n",
    "\n",
    "    padded_train = np.expand_dims(padded_train, -1)\n",
    "    padded_test = np.expand_dims(padded_test, -1)\n",
    "\n",
    "    model.predict(padded_train)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Model run**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model_start(padded_train, padded_test, train_labels, test_labels, model)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
