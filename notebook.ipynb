{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Load Libs**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "# Native python libs\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from typing import Any, NoReturn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "# pip installed libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import sklearn\n",
    "import kerastuner_tensorboard_logger as kt_logger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Paths**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "BASE_PATH = f\"{os.path.abspath('')}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Kaggle**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "KAGGLE = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "KAGGLE_PATH = \"/kaggle\" if KAGGLE else f\"{BASE_PATH}\\\\kaggle\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "def submission_path_exists() -> str:\n",
    "    directory = f\"{KAGGLE_PATH}\\\\working\\\\{datetime.now().strftime('%d%m%Y')}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Created new output directory for today at '{directory}'\")\n",
    "    return directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "INPUT_PATH = f\"{KAGGLE_PATH}\\\\input\\\\goodreads-books-reviews-290312\"\n",
    "OUTPUT_PATH = submission_path_exists()\n",
    "SUBMISSION_PATH = f\"{OUTPUT_PATH}\\\\{datetime.now().strftime('%H%M%S')}_submission.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tensorboard & General Monitoring**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "TENSORBOARD_LOGS_PATH = f\"{BASE_PATH}\\\\tensorboard_logs\"\n",
    "KERAS_TUNER_MONITOR_PATH = f\"{OUTPUT_PATH}\\\\keras_tuner_monitoring\"\n",
    "MONITOR_PATH = f\"{OUTPUT_PATH}\\\\monitoring.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "# Machine Learning tensorboard paths\n",
    "TENSORBOARD_LOGS_PATH_ML = f\"{TENSORBOARD_LOGS_PATH}\\\\ML\"\n",
    "LINEAR = f\"{TENSORBOARD_LOGS_PATH_ML}\\\\Linear\"\n",
    "MLP = f\"{TENSORBOARD_LOGS_PATH_ML}\\\\MLP\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "# Deep Learning tensorboard paths\n",
    "TENSORBOARD_LOGS_PATH_DL = f\"{TENSORBOARD_LOGS_PATH}\\\\DL\"\n",
    "CNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\CNN\"\n",
    "RESNET = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\ResNet\"\n",
    "RNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\RNN\"\n",
    "SIMPLE_RNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\SimpleRNN\"\n",
    "TRANSFORMER = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\Transformer\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\tensorboard_logs'"
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if path is good\n",
    "os.path.abspath(TENSORBOARD_LOGS_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GPU/TPU MultiThreading Setup**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    strategy = tf.distribute.experimental.TPUStrategy\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on single GPU  /device:GPU:0\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "if tpu:\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu, )\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "elif len(gpus) > 1:\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
    "elif len(gpus) == 1:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on single GPU ', gpus[0].name)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hyperparameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "# Fix\n",
    "CLASSES = 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "# Adjustable\n",
    "BATCH_SIZE = 1024  # Big batch size, small learning rate\n",
    "VOCAB_SIZE = 20000\n",
    "SEQUENCE_LENGTH = 256\n",
    "EMBEDDING_DIMS = 128\n",
    "EPOCHS = 100\n",
    "TRIALS = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load Datasets**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f\"{INPUT_PATH}\\\\goodreads_train.csv\",\n",
    "                            usecols=['review_text', 'rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(f\"{INPUT_PATH}\\\\goodreads_test.csv\",\n",
    "                           usecols=['review_id', 'review_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NLP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                                    standardize=None,\n",
    "                                                    output_sequence_length=SEQUENCE_LENGTH,\n",
    "                                                    output_mode='int')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    vectorize_layer.adapt(train_dataset['review_text'], batch_size=BATCH_SIZE * strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [
    {
     "data": {
      "text/plain": "['the', 'and', 'I', 'to', 'a', 'of', 'is', 'was']"
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()[2:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "def vectorize_text(text: Any, label: Any) -> Any:\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating Dataset For Models**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = sklearn.model_selection.train_test_split(train_dataset, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "review_ids = test_dataset['review_id']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [
    "def dataset_from_raw_data(x: np.ndarray, y: np.ndarray, batch_size: int = BATCH_SIZE) -> Any:\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
    "    # Vectorize\n",
    "    dataset = dataset.map(vectorize_text)\n",
    "    print(dataset.element_spec)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_from_raw_data(train_dataset['review_text'], train_dataset['rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset_from_raw_data(validation_dataset['review_text'], validation_dataset['rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "test_dataset = dataset_from_raw_data(test_dataset['review_text'],\n",
    "                                     np.random.randint(0, 5, len(test_dataset['review_text'])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [],
   "source": [
    "def linear(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(CLASSES, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MLP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "def mlp(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(tf.keras.layers.Dense(units=hp.Int('units_0', min_value=32, max_value=512, step=8),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=8),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(CLASSES, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CNN**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "def cnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32))\n",
    "    model.add(tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS))\n",
    "    model.add(tf.keras.layers.Reshape((math.isqrt(SEQUENCE_LENGTH), math.isqrt(SEQUENCE_LENGTH), -1),\n",
    "                                      input_shape=(None, SEQUENCE_LENGTH)))\n",
    "\n",
    "    # Conv & pooling tf.keras.layers\n",
    "    hp_filters_0 = hp.Int('filters_0', min_value=8, max_value=32, step=8)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_0, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_0, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=16, max_value=64, step=8)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_1, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_1, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_2 = hp.Int('filters_2', min_value=32, max_value=128, step=8)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_2, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_2, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_3 = hp.Int('filters_3', min_value=64, max_value=256, step=8)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_3, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_3, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "    # Fully connected tf.keras.layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    hp_units_0 = hp.Int('units_0', min_value=64, max_value=256, step=8)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_0, activation='relu'))\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=8)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu'))\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=8)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ResNet**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "def residual_module(data,\n",
    "                    filters,\n",
    "                    stride,\n",
    "                    reduce,\n",
    "                    reg,\n",
    "                    bn_eps,\n",
    "                    bn_momentum):\n",
    "    shortcut = 0\n",
    "    bn_1 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(data)\n",
    "    act_1 = tf.keras.layers.ReLU()(bn_1)\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters=int(filters / 4.),\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_1)\n",
    "    bn_2 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(conv_1)\n",
    "    act_2 = tf.keras.layers.ReLU()(bn_2)\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=int(filters / 4.),\n",
    "                                    kernel_size=(3, 3),\n",
    "                                    strides=stride,\n",
    "                                    padding='same',\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_2)\n",
    "    bn_3 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(conv_2)\n",
    "    act_3 = tf.keras.layers.ReLU()(bn_3)\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters=filters,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_3)\n",
    "\n",
    "    if reduce:\n",
    "        shortcut = tf.keras.layers.Conv2D(filters=filters,\n",
    "                                          kernel_size=(1, 1),\n",
    "                                          strides=stride,\n",
    "                                          use_bias=False,\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(reg))(act_1)\n",
    "\n",
    "    x = tf.keras.layers.Add()([conv_3, shortcut])\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [],
   "source": [
    "def resnet(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    filters = [64, 64, 128, 256, 512]\n",
    "    stages = [3, 4, 6, 3]\n",
    "    bn_eps = hp.Float('bn_eps', min_value=1e-8, max_value=1e-2, sampling='LOG')\n",
    "    bn_momentum = hp.Float('momentum', min_value=1e-3, max_value=1e-5)\n",
    "    reg = hp.Float('reg', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,))\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                           epsilon=bn_eps,\n",
    "                                           momentum=bn_momentum)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(filters[0], (3, 3),\n",
    "                               use_bias=False,\n",
    "                               padding='same',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(reg))(x)\n",
    "    for i in range(len(stages)):\n",
    "        stride = (1, 1) if i == 0 else (2, 2)\n",
    "        x = residual_module(data=x, filters=filters[i + 1], stride=stride,\n",
    "                            reduce=True, bn_eps=bn_eps, bn_momentum=bn_momentum)\n",
    "        for j in range(stages[i] - 1):\n",
    "            x = residual_module(data=x,\n",
    "                                filters=filters[i + 1],\n",
    "                                stride=(1, 1),\n",
    "                                bn_eps=bn_eps,\n",
    "                                bn_momentum=bn_momentum)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                           epsilon=bn_eps,\n",
    "                                           momentum=bn_momentum)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((8, 8))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(CLASSES,\n",
    "                                    activation=\"softmax\",\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**RNN**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "def rnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    for i in range(hp.Int('num_layers', min_value=0, max_value=3, step=1)):\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hp.Int('units_' + str(i), min_value=8, max_value=64, step=8),\n",
    "                                 return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(hp.Int('lstm_units', min_value=8, max_value=64, step=8),\n",
    "                             return_sequences=False))(x)\n",
    "    x = tf.keras.layers.Dense(hp.Int('dense_units', min_value=8, max_value=64, step=8),\n",
    "                              activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transformer**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size,\n",
    "                                           num_heads=num_heads,\n",
    "                                           dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "def transformer(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,))\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    for _ in range(hp.Int('num_transformer_blocks', min_value=1, max_value=3, step=1)):\n",
    "        x = transformer_encoder(x,\n",
    "                                hp.Int('head_size', min_value=8, max_value=64, step=8),\n",
    "                                hp.Int('num_heads', min_value=1, max_value=2, step=1),\n",
    "                                hp.Int('ff_dim', min_value=8, max_value=64, step=8),\n",
    "                                hp.Fixed('dropout', value=0))\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in [hp.Int('mlp_units', min_value=8, max_value=64, step=8)]:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(hp.Float('mlp_dropout', min_value=0, max_value=1e-1, step=1e-1))(x)\n",
    "    outputs = tf.keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Utilitary For Monitoring**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [],
   "source": [
    "def path_exists(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        i = 0\n",
    "        while True:\n",
    "            new_path = f\"{path}_{i}\"\n",
    "            if not os.path.exists(new_path):\n",
    "                return new_path\n",
    "            i += 1\n",
    "    return path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "def tensorboard_logs(model_name: str) -> tf.keras.callbacks.TensorBoard:\n",
    "    path = f\"{globals()[model_name.upper()]}\" \\\n",
    "           f\"_BS_{BATCH_SIZE}\" \\\n",
    "           f\"_MAXFEAT_{VOCAB_SIZE}\" \\\n",
    "           f\"_EMBEDDING_{EMBEDDING_DIMS}\" \\\n",
    "           f\"_SEQLEN_{SEQUENCE_LENGTH}\" \\\n",
    "           f\"_EPOCHS_{EPOCHS}\" \\\n",
    "           f\"_TRIALS_{TRIALS}\"\n",
    "    return tf.keras.callbacks.TensorBoard(path_exists(path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [],
   "source": [
    "def epochs_logs(model_name: str) -> tf.keras.callbacks.CSVLogger:\n",
    "    path = f\"{globals()[model_name.upper()]}\" \\\n",
    "           f\"_BS_{BATCH_SIZE}\" \\\n",
    "           f\"_MAXFEAT_{VOCAB_SIZE}\" \\\n",
    "           f\"_EMBEDDING_{EMBEDDING_DIMS}\" \\\n",
    "           f\"_SEQLEN_{SEQUENCE_LENGTH}\" \\\n",
    "           f\"_EPOCHS_{EPOCHS}\" \\\n",
    "           f\"_TRIALS_{TRIALS}\"\n",
    "    return tf.keras.callbacks.CSVLogger(f\"{path_exists(path)}.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training & Hyperparameter Optimization**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [],
   "source": [
    "def optimizer_choice(model: tf.keras.Sequential, model_name: str, optimizer: str) -> Any:\n",
    "    if optimizer == \"RandomSearch\":\n",
    "        tuner = kt.RandomSearch(model,\n",
    "                                objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                                max_trials=TRIALS,\n",
    "                                overwrite=True,\n",
    "                                project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                                directory=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\")\n",
    "    elif optimizer == \"BayesianOptimization\":\n",
    "        tuner = kt.BayesianOptimization(model,\n",
    "                                        objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                                        max_trials=TRIALS,\n",
    "                                        overwrite=True,\n",
    "                                        project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                                        directory=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\")\n",
    "    elif optimizer == \"Hyperband\":\n",
    "        tuner = kt.Hyperband(model,\n",
    "                             objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                             max_epochs=TRIALS,\n",
    "                             overwrite=True,\n",
    "                             project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                             logger=kt_logger.TensorBoardLogger(metrics=['val_accuracy'],\n",
    "                                                                logdir=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\"))\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_choice must be 0, 1 or 2\")\n",
    "\n",
    "    return tuner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "def hp_optimization_and_training(model: Any, optimizer: str = \"Hyperband\") -> NoReturn:\n",
    "    model_name = model.__name__\n",
    "    with strategy.scope():\n",
    "        tuner = optimizer_choice(model, model_name, optimizer=optimizer)\n",
    "\n",
    "        # Search for best hyperparameters\n",
    "        tuner.search(train_dataset,\n",
    "                     epochs=EPOCHS,\n",
    "                     validation_data=validation_dataset,\n",
    "                     callbacks=[stop_early,\n",
    "                                epochs_logs(model_name),\n",
    "                                tensorboard_logs(model_name)])\n",
    "        # Get the optimal hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(best_hps)\n",
    "\n",
    "        # Build model with optimal hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=EPOCHS,\n",
    "                            validation_data=validation_dataset,\n",
    "                            callbacks=[stop_early,\n",
    "                                       epochs_logs(model_name),\n",
    "                                       tensorboard_logs(model_name)])\n",
    "        val_acc_per_epoch = history.history['val_accuracy']\n",
    "        best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "        print(f\"best_epoch : {best_epoch}\")\n",
    "\n",
    "        hypermodel = tuner.hypermodel.build(best_hps)\n",
    "        # Retrain the model with epoch with highest val_accuracy value\n",
    "        hypermodel.fit(train_dataset,\n",
    "                       epochs=best_epoch,\n",
    "                       validation_data=validation_dataset,\n",
    "                       callbacks=[stop_early,\n",
    "                                  epochs_logs(model_name),\n",
    "                                  tensorboard_logs(model_name)])\n",
    "\n",
    "        eval_result = hypermodel.evaluate(validation_dataset)\n",
    "\n",
    "        hypermodel.save(f\"{OUTPUT_PATH}\\\\\"\n",
    "                        f\"{model_name}\"\n",
    "                        f\"_loss_{eval_result[0]}\"\n",
    "                        f\"_acc_{eval_result[1]}\"\n",
    "                        f\"_best_epoch_{best_epoch}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_optimization_and_training(linear, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_optimization_and_training(mlp, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_optimization_and_training(cnn, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_optimization_and_training(resnet, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 09m 14s]\n",
      "val_accuracy: 0.5992555618286133\n",
      "\n",
      "Best val_accuracy So Far: 0.5992555618286133\n",
      "Total elapsed time: 00h 28m 15s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x000002158D3932B0>\n",
      "Epoch 1/5\n",
      "704/704 [==============================] - 113s 156ms/step - loss: 1.0813 - accuracy: 0.5356 - val_loss: 0.9624 - val_accuracy: 0.5875\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 109s 154ms/step - loss: 0.9323 - accuracy: 0.6014 - val_loss: 0.9433 - val_accuracy: 0.5978\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 115s 163ms/step - loss: 0.8859 - accuracy: 0.6224 - val_loss: 0.9610 - val_accuracy: 0.5936\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 118s 168ms/step - loss: 0.8526 - accuracy: 0.6374 - val_loss: 1.0025 - val_accuracy: 0.5849\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 128s 182ms/step - loss: 0.8274 - accuracy: 0.6494 - val_loss: 1.0075 - val_accuracy: 0.5862\n",
      "best_epoch : 2\n",
      "Epoch 1/2\n",
      "704/704 [==============================] - 130s 180ms/step - loss: 1.0675 - accuracy: 0.5417 - val_loss: 0.9606 - val_accuracy: 0.5875\n",
      "Epoch 2/2\n",
      "704/704 [==============================] - 113s 160ms/step - loss: 0.9262 - accuracy: 0.6045 - val_loss: 0.9452 - val_accuracy: 0.5973\n",
      "176/176 [==============================] - 17s 97ms/step - loss: 0.9452 - accuracy: 0.5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanm\\OneDrive\\Bureau\\ESGI - Projets\\4IABD\\Projet Deep Learning\\kaggle\\working\\26022023\\rnn_loss_0.9452013969421387_acc_0.5972722172737122_best_epoch_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanm\\OneDrive\\Bureau\\ESGI - Projets\\4IABD\\Projet Deep Learning\\kaggle\\working\\26022023\\rnn_loss_0.9452013969421387_acc_0.5972722172737122_best_epoch_2\\assets\n"
     ]
    }
   ],
   "source": [
    "hp_optimization_and_training(rnn, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hp_optimization_and_training(transformer, optimizer=\"BayesianOptimization\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\rnn_loss_0.9452013969421387_acc_0.5972722172737122_best_epoch_2', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\rnn_loss_0.9616591334342957_acc_0.5904111266136169_best_epoch_2', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\rnn_loss_0.9633556604385376_acc_0.5894277691841125_best_epoch_2', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\25022023\\\\rnn_loss_0.9779065847396851_acc_0.5878055691719055_best_epoch_3', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\23022023\\\\mlp_loss_1.0539891719818115_acc_0.5459833145141602', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\25022023\\\\transformer_loss_1.0704385042190552_acc_0.5419889092445374_best_epoch_2', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\linear_loss_1.0948423147201538_acc_0.5397722125053406', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\19022023\\\\linear_loss_1.0978895425796509_acc_0.5394444465637207', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\19022023\\\\mlp_loss_1.073839545249939_acc_0.5354999899864197', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\mlp_loss_1.074715495109558_acc_0.5347222089767456', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\18022023\\\\linear_loss_1.113369345664978_acc_0.5319499969482422', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\19022023\\\\cnn_loss_1.1353473663330078_acc_0.5252388715744019', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\mlp_loss_1.342788577079773_acc_0.5202833414077759', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\cnn_loss_1.1665804386138916_acc_0.5159666538238525_para_2_32_16_16', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\25022023\\\\rnn_loss_1.2226916551589966_acc_0.5072000026702881_best_epoch_3', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\mlp_loss_1.167117714881897_acc_0.49986666440963745', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\26022023\\\\cnn_loss_1.5174003839492798_acc_0.49327778816223145', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.2623987197875977_acc_0.4714333415031433', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.7791904211044312_acc_0.3497055470943451', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.7717708349227905_acc_0.34896111488342285', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.499313235282898_acc_0.34820556640625', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.5017057657241821_acc_0.34798333048820496', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\23022023\\\\simple_rnn_loss_1.4871493577957153_acc_0.3476555645465851', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\22022023\\\\simple_rnn_loss_1.817870855331421_acc_0.33559998869895935', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.749653935432434_acc_0.323072224855423', 'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle/working\\\\24022023\\\\linear_loss_1.7628369331359863_acc_0.29775556921958923']\n"
     ]
    }
   ],
   "source": [
    "models = [f'{root}\\\\{dir}' for root, dirs, files in os.walk(f'{KAGGLE_PATH}/working') for dir in dirs if \"acc\" in dir]\n",
    "sort_models_per_acc = sorted(models,\n",
    "                             key=lambda x: float(x[x.find('_acc_') + 5:x.find('_best_') if 'best' in x else x.find('_para_') if \"_para_\" in x else None]),\n",
    "                             reverse=True)\n",
    "print(sort_models_per_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def models_acc(model_name):\n",
    "    model = [res[res.find(model_name):] for res in sort_models_per_acc if model_name in res]\n",
    "    print(model[0] if len(model) != 0 else f\"No {model_name} model found\")\n",
    "    print(model[1] if len(model) > 0 else f\"No {model_name} model found\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_acc_and_loss = []\n",
    "for model_name in [\"linear\", \"mlp\", \"cnn\", \"resnet\", \"rnn\", \"transformer\"]:\n",
    "    models_acc(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_acc_and_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(sort_models_per_acc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467/467 [==============================] - 43s 88ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(test_dataset).argmax(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Submission**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['review_id'] = [data for data in review_ids]\n",
    "submission['rating'] = predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission registered at C:\\Users\\juanm\\OneDrive\\Bureau\\ESGI - Projets\\4IABD\\Projet Deep Learning\\kaggle\\working\\26022023\\223812_submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Submission registered at {SUBMISSION_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [
    {
     "data": {
      "text/plain": "4    187670\n5    148699\n3     83640\n2     38818\n1      9704\n0      9502\nName: rating, dtype: int64"
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "4    187670\n5    148699\n3     83640\n2     38818\n1      9704\n0      9502\nName: rating, dtype: int64"
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(SUBMISSION_PATH)\n",
    "test['rating'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "data": {
      "text/plain": "4    192131\n5    124705\n3    107315\n2     26796\n0     15154\n1     11932\nName: rating, dtype: int64"
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = pd.read_csv(\"C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\kaggle\\\\working\\\\25022023\\\\145908_submission.csv\")\n",
    "reference['rating'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
