{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Load Libs**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# Native python libs\n",
    "import os\n",
    "import math\n",
    "from functools import lru_cache\n",
    "from datetime import datetime\n",
    "from typing import Any, Union, NoReturn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# pip installed libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import sklearn\n",
    "import kerastuner_tensorboard_logger as kt_logger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Paths**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "BASE_PATH = f\"{os.path.abspath('')}\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Kaggle**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "KAGGLE = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "KAGGLE_PATH = \"/kaggle\" if KAGGLE else f\"{BASE_PATH}\\\\kaggle\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def submission_path_exists() -> str:\n",
    "    directory = f\"{KAGGLE_PATH}\\\\working\\\\{datetime.now().strftime('%d%m%Y')}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "        print(f\"Created new output directory for today at '{directory}'\")\n",
    "    return directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "INPUT_PATH = f\"{KAGGLE_PATH}\\\\input\\\\goodreads-books-reviews-290312\"\n",
    "OUTPUT_PATH = submission_path_exists()\n",
    "SUBMISSION_PATH = f\"{OUTPUT_PATH}\\\\{datetime.now().strftime('%H%M%S')}_submission.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tensorboard & General Monitoring**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "TENSORBOARD_LOGS_PATH = f\"{BASE_PATH}\\\\tensorboard_logs\"\n",
    "KERAS_TUNER_MONITOR_PATH = f\"{OUTPUT_PATH}\\\\keras_tuner_monitoring\"\n",
    "MONITOR_PATH = f\"{OUTPUT_PATH}\\\\monitoring.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Machine Learning tensorboard paths\n",
    "TENSORBOARD_LOGS_PATH_ML = f\"{TENSORBOARD_LOGS_PATH}\\\\ML\"\n",
    "LINEAR = f\"{TENSORBOARD_LOGS_PATH_ML}\\\\Linear\"\n",
    "MLP = f\"{TENSORBOARD_LOGS_PATH_ML}\\\\MLP\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# Deep Learning tensorboard paths\n",
    "TENSORBOARD_LOGS_PATH_DL = f\"{TENSORBOARD_LOGS_PATH}\\\\DL\"\n",
    "CNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\CNN\"\n",
    "RESNET = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\ResNet\"\n",
    "RNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\RNN\"\n",
    "SIMPLE_RNN = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\SimpleRNN\"\n",
    "TRANSFORMER = f\"{TENSORBOARD_LOGS_PATH_DL}\\\\Transformer\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\juanm\\\\OneDrive\\\\Bureau\\\\ESGI - Projets\\\\4IABD\\\\Projet Deep Learning\\\\tensorboard_logs'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if path is good\n",
    "os.path.abspath(TENSORBOARD_LOGS_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hyperparameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Fix\n",
    "CLASSES = 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "# Adjustable\n",
    "BATCH_SIZE = 1024  # Big batch size, small learning rate\n",
    "VOCAB_SIZE = 20000\n",
    "SEQUENCE_LENGTH = 256\n",
    "EMBEDDING_DIMS = 128\n",
    "EPOCHS = 100\n",
    "TRIALS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load Datasets**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(f\"{INPUT_PATH}\\\\goodreads_train.csv\",\n",
    "                            usecols=['review_text', 'rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(f\"{INPUT_PATH}\\\\goodreads_test.csv\",\n",
    "                           usecols=['review_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GPU/TPU MultiThreading Setup**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "    strategy = tf.distribute.experimental.TPUStrategy\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on single GPU  /device:GPU:0\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "if tpu:\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu, )\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "elif len(gpus) > 1:\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
    "elif len(gpus) == 1:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on single GPU ', gpus[0].name)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NLP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                                    standardize=None,\n",
    "                                                    output_sequence_length=SEQUENCE_LENGTH,\n",
    "                                                    output_mode='int')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20.2 s\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    vectorize_layer.adapt(train_dataset['review_text'], batch_size=BATCH_SIZE * strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "'to'"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()[5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "def vectorize_text(text: Any, label: Any) -> Any:\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating Dataset For Models**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = sklearn.model_selection.train_test_split(train_dataset, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def dataset_from_raw_data(x: np.ndarray, y: np.ndarray, batch_size: int = BATCH_SIZE) -> Any:\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
    "    # Vectorize\n",
    "    dataset = dataset.map(vectorize_text)\n",
    "    print(dataset.element_spec)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset_from_raw_data(train_dataset['review_text'], train_dataset['rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset_from_raw_data(validation_dataset['review_text'], validation_dataset['rating'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def linear(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(CLASSES, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**MLP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def mlp(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(tf.keras.layers.Dense(units=hp.Int('units_0', min_value=32, max_value=512, step=32),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(CLASSES, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CNN**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def cnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32))\n",
    "    model.add(tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS))\n",
    "    model.add(tf.keras.layers.Reshape((math.isqrt(SEQUENCE_LENGTH), math.isqrt(SEQUENCE_LENGTH), -1),\n",
    "                                      input_shape=(None, SEQUENCE_LENGTH)))\n",
    "\n",
    "    # Conv & pooling tf.keras.layers\n",
    "    hp_filters_0 = hp.Int('filters_0', min_value=8, max_value=32, step=8)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_0, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_0, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=16, max_value=64, step=16)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_1, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_1, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_2 = hp.Int('filters_2', min_value=32, max_value=128, step=32)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_2, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_2, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "    hp_filters_3 = hp.Int('filters_3', min_value=64, max_value=256, step=64)\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_3, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=hp_filters_3, kernel_size=(3, 3), activation='tanh', padding='same'))\n",
    "    model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "    # Fully connected tf.keras.layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    hp_units_0 = hp.Int('units_0', min_value=64, max_value=256, step=64)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_0, activation='relu'))\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu'))\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=16)\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ResNet**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def residual_module(data,\n",
    "                    filters,\n",
    "                    stride,\n",
    "                    reduce,\n",
    "                    reg,\n",
    "                    bn_eps,\n",
    "                    bn_momentum):\n",
    "    shortcut = 0\n",
    "    bn_1 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(data)\n",
    "    act_1 = tf.keras.layers.ReLU()(bn_1)\n",
    "    conv_1 = tf.keras.layers.Conv2D(filters=int(filters / 4.),\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_1)\n",
    "    bn_2 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(conv_1)\n",
    "    act_2 = tf.keras.layers.ReLU()(bn_2)\n",
    "    conv_2 = tf.keras.layers.Conv2D(filters=int(filters / 4.),\n",
    "                                    kernel_size=(3, 3),\n",
    "                                    strides=stride,\n",
    "                                    padding='same',\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_2)\n",
    "    bn_3 = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                              epsilon=bn_eps,\n",
    "                                              momentum=bn_momentum)(conv_2)\n",
    "    act_3 = tf.keras.layers.ReLU()(bn_3)\n",
    "    conv_3 = tf.keras.layers.Conv2D(filters=filters,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(reg))(act_3)\n",
    "\n",
    "    if reduce:\n",
    "        shortcut = tf.keras.layers.Conv2D(filters=filters,\n",
    "                                          kernel_size=(1, 1),\n",
    "                                          strides=stride,\n",
    "                                          use_bias=False,\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(reg))(act_1)\n",
    "\n",
    "    x = tf.keras.layers.Add()([conv_3, shortcut])\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def resnet(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,))\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(x)\n",
    "    x = tf.keras.layers.Reshape((SEQUENCE_LENGTH,))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                           epsilon=bn_eps,\n",
    "                                           momentum=bn_momentum)(inputs)\n",
    "    x = tf.keras.layers.Conv2D(filters[0], (3, 3),\n",
    "                               use_bias=False,\n",
    "                               padding='same',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(reg))(x)\n",
    "    for i in range(len(stages)):\n",
    "        stride = (1, 1) if i == 0 else (2, 2)\n",
    "        x = residual_module(data=x, filters=filters[i + 1], stride=stride,\n",
    "                            reduce=True, bn_eps=bn_eps, bn_momentum=bn_momentum)\n",
    "        for j in range(stages[i] - 1):\n",
    "            x = residual_module(data=x,\n",
    "                                filters=filters[i + 1],\n",
    "                                stride=(1, 1),\n",
    "                                bn_eps=bn_eps,\n",
    "                                bn_momentum=bn_momentum)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1,\n",
    "                                           epsilon=bn_eps,\n",
    "                                           momentum=bn_momentum)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((8, 8))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(CLASSES, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(reg))(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**RNN**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def rnn(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=3, step=1)):\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hp.Int('units_' + str(i), min_value=32, max_value=512, step=1),\n",
    "                                 return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(hp.Int('lstm_units', min_value=16, max_value=64, step=1),\n",
    "                             return_sequences=False))(x)\n",
    "    x = tf.keras.layers.Dense(hp.Int('dense_units', min_value=16, max_value=64, step=1),\n",
    "                                  activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def rnn_test() -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    for i in range(2):\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(32, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(16, return_sequences=False))(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Transformer**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def transformer(hp: kt.HyperParameters) -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,))\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = tf.keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def transformer_test() -> tf.keras.Sequential:\n",
    "    inputs = tf.keras.Input(shape=(SEQUENCE_LENGTH,))\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE + 1, EMBEDDING_DIMS)(inputs)\n",
    "    for _ in range(2):\n",
    "        x = transformer_encoder(x, 32, 2, 32, 0.1)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in [32]:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    outputs = tf.keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-2),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 256, 128)     2560128     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 256, 128)    256         ['embedding_5[0][0]']            \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 256, 128)    33088       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 256, 128)     0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 256, 128)    0           ['dropout_17[0][0]',             \n",
      " ambda)                                                           'embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 256, 128)    256         ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 256, 32)      4128        ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 256, 32)      0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 256, 128)     4224        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 256, 128)    0           ['conv1d_15[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 256, 128)    256         ['tf.__operators__.add_15[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (MultiH  (None, 256, 128)    33088       ['layer_normalization_16[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 256, 128)     0           ['multi_head_attention_8[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 256, 128)    0           ['dropout_19[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_17 (LayerN  (None, 256, 128)    256         ['tf.__operators__.add_16[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 256, 32)      4128        ['layer_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 256, 32)      0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 256, 128)     4224        ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 256, 128)    0           ['conv1d_17[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 256)         0           ['tf.__operators__.add_17[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 32)           8224        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 32)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 6)            198         ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,652,454\n",
      "Trainable params: 2,652,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_5/dropout_19/dropout/Mul' defined at (most recent call last):\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\juanm\\AppData\\Local\\Temp\\ipykernel_45800\\3957619528.py\", line 4, in <cell line: 3>\n      model.fit(train_dataset, epochs=10, validation_data=validation_dataset)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'model_5/dropout_19/dropout/Mul'\nfailed to allocate memory\n\t [[{{node model_5/dropout_19/dropout/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_97170]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[1;32mIn [125]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39msummary()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: Graph execution error:\n\nDetected at node 'model_5/dropout_19/dropout/Mul' defined at (most recent call last):\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\juanm\\AppData\\Local\\Temp\\ipykernel_45800\\3957619528.py\", line 4, in <cell line: 3>\n      model.fit(train_dataset, epochs=10, validation_data=validation_dataset)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\juanm\\.conda\\envs\\DeepLearning4IABD\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'model_5/dropout_19/dropout/Mul'\nfailed to allocate memory\n\t [[{{node model_5/dropout_19/dropout/Mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_97170]"
     ]
    }
   ],
   "source": [
    "model = transformer_test()\n",
    "model.summary()\n",
    "with strategy.scope():\n",
    "    model.fit(train_dataset, epochs=10, validation_data=validation_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Utilitary For Monitoring**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def tensorboard_logs(model_name: str) -> tf.keras.callbacks.TensorBoard:\n",
    "    return tf.keras.callbacks.TensorBoard(f\"{globals()[model_name.upper()]}\"\n",
    "                                          f\"_BS_{BATCH_SIZE}\"\n",
    "                                          f\"_MAXFEAT_{VOCAB_SIZE}\"\n",
    "                                          f\"_EMBEDDING_{EMBEDDING_DIMS}\"\n",
    "                                          f\"_SEQLEN_{SEQUENCE_LENGTH}\"\n",
    "                                          f\"_EPOCHS_{EPOCHS}\"\n",
    "                                          f\"_TRIALS_{TRIALS}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training & Hyperparameter Optimization**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def optimizer_choice(model: tf.keras.Sequential, model_name: str, optimizer: str):\n",
    "    if optimizer == \"RandomSearch\":\n",
    "        tuner = kt.RandomSearch(model,\n",
    "                                objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                                max_trials=TRIALS,\n",
    "                                overwrite=True,\n",
    "                                project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                                directory=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\")\n",
    "    elif optimizer == \"BayesianOptimization\":\n",
    "        tuner = kt.BayesianOptimization(model,\n",
    "                                        objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                                        max_trials=TRIALS,\n",
    "                                        overwrite=True,\n",
    "                                        project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                                        directory=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\")\n",
    "    elif optimizer == \"Hyperband\":\n",
    "        tuner = kt.Hyperband(model,\n",
    "                             objective=kt.Objective('val_accuracy', direction='max'),\n",
    "                             max_epochs=TRIALS,\n",
    "                             overwrite=True,\n",
    "                             project_name=f\"{OUTPUT_PATH}\\\\{model_name}_tuner\",\n",
    "                             logger=kt_logger.TensorBoardLogger(metrics=['val_accuracy'],\n",
    "                                                                logdir=f\"{KERAS_TUNER_MONITOR_PATH}_{model_name}\"))\n",
    "    else:\n",
    "        raise ValueError(\"optimizer_choice must be 0, 1 or 2\")\n",
    "\n",
    "    return tuner"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def hp_optimization_and_training(model: Any, optimizer: str = \"Hyperband\") -> NoReturn:\n",
    "    model_name = model.__name__\n",
    "    with strategy.scope():\n",
    "        tuner = optimizer_choice(model, model_name, optimizer=optimizer)\n",
    "\n",
    "        # Search for best hyperparameters\n",
    "        tuner.search(train_dataset,\n",
    "                     epochs=EPOCHS,\n",
    "                     validation_data=validation_dataset,\n",
    "                     callbacks=[stop_early,\n",
    "                                tensorboard_logs(model_name)])\n",
    "        # Get the optimal hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(best_hps)\n",
    "\n",
    "        # Build model with optimal hyperparameters\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=EPOCHS,\n",
    "                            validation_data=validation_dataset,\n",
    "                            callbacks=[stop_early,\n",
    "                                       tensorboard_logs(model_name)])\n",
    "        val_acc_per_epoch = history.history['val_accuracy']\n",
    "        best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "        print(f\"best_epoch : {best_epoch}\")\n",
    "\n",
    "        hypermodel = tuner.hypermodel.build(best_hps)\n",
    "        # Retrain the model with epoch with highest accuracy value\n",
    "        hypermodel.fit(train_dataset,\n",
    "                       epochs=best_epoch,\n",
    "                       callbacks=[stop_early,\n",
    "                                  tensorboard_logs(model_name)])\n",
    "\n",
    "        eval_result = hypermodel.evaluate(validation_dataset)\n",
    "\n",
    "        hypermodel.save(f\"{OUTPUT_PATH}\\\\\"\n",
    "                        f\"{model_name}\"\n",
    "                        f\"_loss_{eval_result[0]}\"\n",
    "                        f\"_acc_{eval_result[1]}\"\n",
    "                        f\"_best_epoch_{best_epoch}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = [dir for root, dirs, files in os.walk(f'{KAGGLE_PATH}/working') for dir in dirs if \"acc\" in dir]\n",
    "sort_models_per_acc = sorted(models, key=lambda x: float(x[x.find('_acc_') + 5:]), reverse=True)\n",
    "sort_models_per_loss = sorted(models, key=lambda x: float(x[x.find('_loss_') + 6:x.find('_acc_')]))\n",
    "print(sort_models_per_acc)\n",
    "print(sort_models_per_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(f\"{KAGGLE_PATH}/working/{sort_models_per_acc[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Submission**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['review_id'] = [data.decode(\"utf-8\") for data in test_dataset['review_id']]\n",
    "submission['rating'] = best_model.predict(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Submission registered at {SUBMISSION_PATH}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
